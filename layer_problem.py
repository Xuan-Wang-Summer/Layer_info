# -*- coding: utf-8 -*-
"""layer_problem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O4v25ojswiaQIrCbNnkcrFI0WHlSmjnT
"""

import torch
import torch.nn as nn
from torch.autograd import Variable
from graphviz import Digraph
from collections import OrderedDict
import torch.nn.functional as F
import numpy as np
from torchvision import models
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union
import os
import shutil
from torch.jit import ScriptModule
import torchvision
import torchvision.transforms as transforms
import sys

# Holds the information for the information for the layer
# weight_h = R, weight_w = S, output_h = P, output_w = Q, num_in_chan = C,
# num_out_chan = K, input_size = N, ln_feature = L)
class layer_pr:
  def __init__(self,
               layer_name: str,
               module: nn.Module,
               weight_h: int,
               weight_w: int,
               output_h: int,
               output_w: int,
               num_in_chan: int,
               num_out_chan: int,
               batch_size: int,
               Wstride: int,
               Hstride: int,
               Wdilation: int,
               Hdilation: int,
               layer_depth: int, # whether a layer is belong to sequential...
               depth_index: int
  ):
    self.layer_id = id(module)
    self.class_name = (
                      str(module.original_name)
                      if isinstance(module, ScriptModule)
                      else module.__class__.__name__
    )
    self.module = module
    self.weight_h = weight_h
    self.weight_w = weight_w
    self.output_h = output_h
    self.output_w = output_w
    self.num_in_chan = num_in_chan
    self.num_out_chan = num_out_chan
    self.batch_size = batch_size
    self.layer_name = layer_name
    self.Wstride = Wstride
    self.Hstride = Hstride
    self.Wdilation = Wdilation
    self.Hdilation = Hdilation
    self.layer_depth = layer_depth
    self.depth_index = depth_index

  # Get the name of each layer
  def get_name(self) -> str:
    layer_name = self.class_name
    # layer_name += f" ({self.layer_name})"
    # TODO: include layer_depth
    if self.layer_depth > 0:
      layer_name += f": {self.layer_depth}"
      layer_name += f"-{self.depth_index }"
    return layer_name

  # Write layer's information to a file
  def wr_layer(self, name, num, temp_name):
    # Create filename according to layer depth
    fname = str(num)
    # fname += f": {name}"
    filename = "%s.yaml" % fname
    # Create the correspond new file
    f_layer = open(filename, "w+")
    # Copy from the template
    f_temp = open(temp_name, "r")
    line = f_temp.readline()
    while line:
      f_layer.write(line)
      line = f_temp.readline()
    f_temp.close()
    f_layer.write("\n")
    # Write the content in the file
    f_layer.write("# " + name + "\n")
    f_layer.write("problem:" + "\n")
    f_layer.write("    " + "R: " + str(self.weight_h) + "\n")
    f_layer.write("    " + "S: " + str(self.weight_w) + "\n")
    f_layer.write("    " + "P: " + str(self.output_h) + "\n")
    f_layer.write("    " + "Q: " + str(self.output_w) + "\n")
    f_layer.write("    " + "C: " + str(self.num_in_chan) + "\n")
    f_layer.write("    " + "K: " + str(self.num_out_chan) + "\n")
    f_layer.write("    " + "N: " + str(self.batch_size) + "\n")
    f_layer.write("    " + "Wstride: " + str(self.Wstride) + "\n")
    f_layer.write("    " + "Hstride: " + str(self.Hstride) + "\n")
    f_layer.write("    " + "Wdilation: " + str(self.Wdilation) + "\n")
    f_layer.write("    " + "Hdilation: " + str(self.Hdilation) + "\n")
    # Close file
    f_layer.close()
    # Move the file to the destination directory
    source = filename
    destination = './layer-info'
    dest = shutil.move(source, destination)

# Hold the information to create dot
# class dot_info:
#  def _init_(self,
#             block_name: str,
#             module: nn.Module, )

def summary(model, input, temp_name, device=torch.device('cuda:0')):
  # Create a new directory to store the files for layers
  isDirL = os.path.isdir("./layer-info")
  if isDirL is False:
    directoryL = "layer-info"
    os.mkdir(directoryL)
  # Create a new directory to store the files for block information
  isDirB = os.path.isdir("./block-info")
  if isDirB is False:
    directoryB = "block-info"
    os.mkdir(directoryB)
  # Create an dict to store the index of each file containing layer info
  index = dict({1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0})
  store = []
  # Get results
  model_name = model.__class__.__name__
  named_module = (model_name, model)
  apply_hooks(store, named_module, model, index, temp_name)

# Use hooks to get and write layer info
def apply_hooks(
  store: [],
  named_module: Tuple[str, nn.Module],
  orig_model: nn.Module,
  idx: Dict[int, int],
  temp_name: str,
  curr_depth: int = 0,
  ):
  var_name, module = named_module

  # Return true if module has child
  def check_child(module) -> bool:
    for child in module.named_children():
      var_name_child, module_child = child
      if var_name_child is not None:
        return True
    else:
      return False

  def pre_hook(module, input):
    # Update index
    idx[curr_depth] = idx.get(curr_depth) + 1
    info_name = module.__class__.__name__ # Get the name for each layer
    if check_child(module):
        fname = info_name
        fname += f": {curr_depth}"
        fname += f"-{idx[curr_depth]}"
        filename = "%s.txt" % fname
        # Create the correspond new file
        f_par = open(filename, "w+")
        for child in module.children():
          var_name_child = child.__class__.__name__
          f_par.write(var_name_child + "\n")
        f_par.close()
        # Move the file to the destination directory
        source = filename
        destination = './block-info'
        dest = shutil.move(source, destination)

  def hook(self, input, output):
    info_name = module.__class__.__name__ # Get the name for each layer
    # Check module's weight
    # Check whether a module has a weight attribute
    if hasattr(module, "weight") and hasattr(module.weight, "size"):
      weight_size = module.weight.size()    # Get the size of weight
      info_weight_h = weight_size[0]        # Get height of the weight
      # Check the length of weight size
      if (len(weight_size) > 1) :
        info_weight_w = weight_size[1]        # Get width of the weight
      # Width and height are the same if weight size is 1
      else:
        info_weight_w = weight_size[0]
    # Assign "0"s if the module does not have a weight
    else:
      info_weight_h = 1
      info_weight_w = 1
    # Get the output size
    output_size = output.data.size()
    # Check the length of output size
    if (len(output_size) > 2) :
      info_output_h = output_size[2]        # Get height of the output
      info_output_w = output_size[3]        # Get width of the output
      info_num_out_chan = output_size[1]    # Get the number of output channels
    else:
      info_num_out_chan = 1                 # Set num_out_chan to "1"
      info_output_w = output_size[1]
      info_output_h = 1                    # Assign "1" for Linear module
    # Get the input size
    input_size = input[0].data.size()
    info_num_in_chan = input_size[1]        # Get the number of input channels
    info_batch_size = input_size[0]         # Get batch size

    # Check the nn.BatchNorm2d module
    if isinstance(module, nn.BatchNorm2d):
      info_weight_h = 1
      info_weight_w = 1
      info_num_in_chan = 1

    # Check the ReLu module
    if isinstance(module, nn.ReLU):
      info_weight_h = 1
      info_weight_w = 1
      info_num_in_chan = 1

    # Check the Linear module
    if isinstance(module, nn.Linear):
      info_weight_h = 1
      info_weight_w = 1
      info_output_h = 1
      info_output_w = 1
      info_batch_size = 1
      info_num_in_chan = module.in_features
      info_num_out_chan = module.out_features

    if isinstance(module, nn.MaxUnpool2d):
      #info_weight_h = 1
      #info_weight_w = 1
      info_num_in_chan = 1

    if isinstance(module, nn.AdaptiveAvgPool2d):
      info_weight_h = 1
      info_weight_w = 1
      info_num_in_chan = 1

    # Check whether the module has stride
    if hasattr(module, "stride"):
      layer_stride = module.stride          # Get the stride
      # Check the type of the stride
      if isinstance(layer_stride, tuple):
        info_Hstride = layer_stride[0]      # Get the height of the stride
        info_Wstride = layer_stride[1]      # Get the width of the stride
      # Width and height are the same if stride is int
      else:
        info_Wstride = layer_stride
        info_Hstride = layer_stride
    # Assign "-1"s if the module does not have a stride
    else:
      info_Wstride = 1
      info_Hstride = 1
    # Check whether the module has dilation
    if hasattr(module, "dilation"):
      layer_dilation = module.dilation      # Get the dilation
      # Check the type of the stride
      if isinstance(layer_dilation, tuple):
        info_Hdilation = layer_dilation[0]      # Get the height of the dilation
        info_Wdilation = layer_dilation[1]      # Get the width of the dilation
      # Width and height are the same if dilation is int
      else:
        info_Wdilation = layer_dilation
        info_Hdilation = layer_dilation
    # Assign "-1"s if the module does not have a stride
    else:
      info_Wdilation = 1
      info_Hdilation = 1

    #print(info_name, info_weight_h, info_weight_w, info_output_h, info_output_w)
    # Create the layer_info for the layer
    info = layer_pr(info_name,
                    module,
                    info_weight_h,
                    info_weight_w,
                    info_output_h,
                    info_output_w,
                    info_num_in_chan,
                    info_num_out_chan,
                    info_batch_size,
                    info_Wstride,
                    info_Hstride,
                    info_Wdilation,
                    info_Hdilation,
                    curr_depth,
                    idx[curr_depth],
                    )
    # Print the layer into a file
    file_name = module.__class__.__name__
    file_name += f": {curr_depth}"
    file_name += f"-{idx.get(curr_depth)}"
    store.append(info)
    number = len(store)
    info.wr_layer(file_name, number, temp_name)

  submodules = [m for m in module.modules() if m is not orig_model]
  WRAPPER_MODULES = (nn.ParameterList, nn.ModuleList, ScriptModule)
  if module != orig_model or isinstance(module, nn.MultiheadAttention) or not submodules:
    if isinstance(module, WRAPPER_MODULES):
      pre_hook(module, None)
    else:
      if check_child(module):
        module.register_forward_pre_hook(pre_hook)
      else:
        module.register_forward_pre_hook(pre_hook)
        module.register_forward_hook(hook)

  for child in module.named_children():
    apply_hooks(
        store, child, orig_model, idx, temp_name, curr_depth + 1,
    )

# Change the following line to use different models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # PyTorch v0.4.0
# Change models
model = torchvision.models.resnet18().to(device)
# Change inputs
example = torch.rand(1, 3, 224, 224)
example = example.to(device)
# Assign templates
temp_name = sys.argv[1]
# Run the anaylsis
summary(model, example, temp_name)
out = model(example)
